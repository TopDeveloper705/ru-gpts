# ruGPT3XL, ruGPT3Large, ruGPT3Medium, ruGPT3Small and ruGPT2Large
Russian GPT trained with 2048 context length (ruGPT3XL) with sparse attention, Russian GPT trained with 2048 context length (ruGPT3Large), Russian GPT Medium trained with context 2048 (ruGPT3Medium), Russian GPT Small trained with context 2048 (ruGPT3Small) and Russian GPT2 large (ruGPT2Large) trained with 1024 context length.

We suggest you use ruGPT2Large or ruGPT3XL because this model is more stable and tested.

Examples [here](examples/)

Table of contents

# Christofari GPUs

The organizers gave participants the opportunity to get access to Cristofari by SberCloud.

# Setup and usage
Models can be used for inference or finetuning with two ways: ü§óHuggingFace interface or our code based on this [implementation](https://github.com/microsoft/DeepSpeedExamples/tree/master/Megatron-LM).

For both ways install transformers:

```bash
pip install transformers==3.5.0
```

## HuggingFace interface
We support ü§óHuggingFace interface only for ruGPT3Large, ruGPT3Medium, ruGPT3Small and ruGPT2Large models. For RuGPT3XL please use code in this repo because RuGPT3XL model was trained with sparse attention.

Here we can obtain examples of [finetuning](examples/Finetune_RuGPTs_with_HF.ipynb) or [generation](examples/Generate_text_with_RuGPTs_HF.ipynb).

Also this examples is adapted for google colab:
* [![finetuning](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/Finetune_RuGPTs_with_HF.ipynb)
* [![generation](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/Generate_text_with_RuGPTs_HF.ipynb)

Basic usage:

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer


model_name_or_path = "sberbank-ai/rugpt3large_based_on_gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)
model = GPT2LMHeadModel.from_pretrained(model_name_or_path).cuda()
text = "–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –°–µ—Ä–≥–µ–µ–≤–∏—á –ü—É—à–∫–∏–Ω —Ä–æ–¥–∏–ª—Å—è –≤ "
input_ids = tokenizer.encode(text, return_tensors="pt").cuda()
out = model.generate(input_ids.cuda())
generated_text = list(map(tokenizer.decode, out))[0]
print(generated_text)
# Output should be like this:
# –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –°–µ—Ä–≥–µ–µ–≤–∏—á –ü—É—à–∫–∏–Ω —Ä–æ–¥–∏–ª—Å—è –≤ \n1799 –≥–æ–¥—É. –ï–≥–æ –æ—Ç–µ—Ü –±—ã–ª –∫—Ä–µ–ø–æ—Å—Ç–Ω—ã–º –∫—Ä–µ—Å—Ç—å—è–Ω–∏–Ω–æ–º, –∞ –º–∞—Ç—å ‚Äì –∫—Ä–µ–ø–æ—Å—Ç–Ω–æ–π –∫—Ä–µ—Å—Ç—å—è–Ω–∫–æ–π. –î–µ—Ç—Å—Ç–≤–æ –∏ —é–Ω–æ—Å—Ç—å –ü—É—à–∫–∏–Ω–∞ –ø—Ä–æ—à–ª–∏ –≤ –¥–µ—Ä–µ–≤–Ω–µ –ú–∏—Ö–∞–π–ª–æ–≤—Å–∫–æ–µ –ø–æ–¥ –ü–µ—Ç–µ—Ä–±—É—Ä–≥–æ–º. –í 1820-—Ö –≥–æ–¥–∞—Ö —Å–µ–º—å—è –ø–µ—Ä–µ–µ—Ö–∞–ª–∞
```

For more information about ü§óHuggingFace interface please follow this [documentation](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate).

#### Data issues
For training pass single txt file.

## Megatron interface
### Without deepspeed
For using our code for finetuning without deepspeed (not recommended) we should install apex:

```bash
%%writefile setup.sh

export CUDA_HOME=/usr/local/cuda-10.1
git clone https://github.com/NVIDIA/apex
pip install -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./apex

sh setup.sh
```

Example of finetuning, generating and loading/convert megatron checkpoints [here](examples/Finetune_and_generate_RuGPTs_only_with_megatron.ipynb) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/Finetune_and_generate_RuGPTs_only_with_megatron.ipynb)

**Note!** This way is valid for all RuGPTs models except RuGPT3XL.

### Megatron with deepspeed
For using our code for finetuning with deepspeed (recommended) we should install apex (see previous section) and deepspeed:

```bash
pip install deepspeed==0.3.7
```

Example of finetuning, generating and loading/convert megatron checkpoints [here](examples/Finetune_and_generate_RuGPTs_deepspeed_megatron.ipynb) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/Finetune_and_generate_RuGPTs_deepspeed_megatron.ipynb)

**Note!** For using deepspeed we should specify environ variable before all your python scripts and run with torch.distributed or mpi:

```
USE_DEEPSPEED=1 python -m torch.distributed.launch --nproc_per_node 1 ru-gpts/pretrain_gpt3.py \
  --train-data-path "train.list" \
  --test-data-path "valid.list" \
  --max-files-per-process 100 \
  --save model \
  --load-huggingface sberbank-ai/rugpt3small_based_on_gpt2 \
  --model-parallel-size 1 \
  --num-layers 12 \
  --hidden-size 768 \
  --num-attention-heads 12 \
  --seq-length 2048 \
  --max-position-embeddings 2048 \
  --fp16 \
  --checkpoint-activations \
  --deepspeed-activation-checkpointing \
  --deepspeed \
  --deepspeed_config ru-gpts/src/deepspeed_config/gpt3_small_2048.json
```

#### Data issues
We use custom implementation of distributed dataset. For training and evaluating we should specify file `file.list` with list of paths to txt files. All files from `file.list` will be splitted between aviable GPUs. The logic of splitting is described by the following code:

```python
shard_size = len(files) // world_size
shard_start = rank * shard_size
shard_end = (rank + 1) * shard_size
files = files[shard_start:shard_end]
```

For more details please see full code of dataset: `src.dataset_rugpt3.RuGpt3TextDataset` and example.

**Note!** This way is valid for all RuGPTs models except RuGPT3XL.






## Setup ruGPT3XL
See all details [here](gw/)

## Setup ruGPT3Large
Code reused from microsoft [implementation](https://github.com/microsoft/DeepSpeedExamples/tree/master/Megatron-LM) of Megatron-LM.
Supports only python3.6.

To use this repo please install the latest supported versions of PyTorch with GPU support. 

Additionally, part of this codebase leverages tensorflow-cpu to (optionally) perform dataloading of TFRecords for GPT training. We recommend creating a virtual environment (to avoid breaking existing tf installations) and install our `requirements.txt`. 

```bash
python -m pip install virtualenv
virtualenv gpt_env
source gpt_env/bin/activate
pip install -r requirements.txt
```

For using of sparse operations in attention additionally install [torch-blocksparse](https://github.com/ptillet/torch-blocksparse):

```bash
source gpt_env/bin/activate
pip install torch-blocksparse
```

Torch-Blocksparse depends on CUDA 10.1 and the [Triton](https://github.com/ptillet/triton) language and compiler, which requires llvm-9.

## Setup ruGPT3Medium
For this model you can use code from microsoft [implementation](https://github.com/microsoft/DeepSpeedExamples/tree/master/Megatron-LM) of Megatron-LM in our repo or use transformers interface. Therefore, you should follow the instructions for ruGPT2Large or ruGPT3Large for installation.

## Setup ruGPT3Small
For this model you can use code from microsoft [implementation](https://github.com/microsoft/DeepSpeedExamples/tree/master/Megatron-LM) of Megatron-LM in our repo or use transformers interface. Therefore, you should follow the instructions for ruGPT2Large or ruGPT3Large for installation.

## Setup ruGPT2Large
This model is smaller and was trained with [transformers==v2.8.0](https://github.com/huggingface/transformers/tree/v2.8.0).
For installing use command:
```bash
pip install transformers
```

# Details of pretraining
All GPUs are  Tesla V100-SXM3 32 Gb.

## Details of pretraining ruGPT3XL
Model was trained on 512 context length with [deepspeed](https://github.com/microsoft/DeepSpeed) and [megatron](https://github.com/NVIDIA/Megatron-LM) code by [SberDevices](https://sberdevices.ru/) team. After that model was finetuned on 2048 context. Note! Model has sparse attention modules.

Total training time took around 10 days on 256 GPUs. Final perplexity on test set is `11.4`.

ü§óHuggingFace model card [link](https://huggingface.co/sberbank-ai/rugpt3xl).

See more details [here](gw/)

## Details of pretraining ruGPT3Large
Model was trained on 1024 context length with transformers by [SberDevices](https://sberdevices.ru/) team on 80B tokens around 3 epochs. After that we finetune this on 2048 context. For load transformers checkpoint use `--load-openai`.

The training process took around two weeks on 8 DGX2 (128 GPUs) for 1024 context and few days on 16 GPUs for 2048 context on [Christophari](https://sbercloud.ru/ru/christofari).

Perplexity is 16 on test set.

You can obtain this model here [GDrive](https://drive.google.com/file/d/1t4xw-nvNLQ8kt9FrWW4bPEgCr45M98vu/view?usp=sharing) [Yandex.Disk](https://yadi.sk/d/X7v84O9jrQ8jJg) [GDrive option-2](https://drive.google.com/file/d/1wtc2iBNTcYrqwOzRyEWYWoVBc9xfsbPP/view?usp=sharing) or use in transformers with model name `sberbank-ai/rugpt3large_based_on_gpt2` (see [usage](#Usage-ruGPT3Large) for details).

ü§óHuggingFace model card [link](https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2)

## Details of pretraining ruGPT3Medium
Model was trained on 1024 context length with transformers by [SberDevices](https://sberdevices.ru/) team on 80B tokens around 3 epoch. After that model was finetuned on 2048 context.

Total training time took around 16 days on 64 GPUs.

You can obtain this model here [GDrive](https://drive.google.com/file/d/1Lb9ILKw0N2ZSEG80QyaCvkp1b2RAw1pC/view?usp=sharing) [Yandex.Disk](https://yadi.sk/d/yE0cw0QIikCPAg) [GDrive option-2](https://drive.google.com/file/d/1gADn4VxDBVrxZ9Wv4bISbDjwCm_3mrDH/view?usp=sharing) or use in transformers with model name `sberbank-ai/rugpt3medium_based_on_gpt2` (see [usage](#Usage-ruGPT3Medium) for details). 

ü§óHuggingFace model card [link](https://huggingface.co/sberbank-ai/rugpt3medium_based_on_gpt2)

## Details of pretraining ruGPT3Small
Model was trained on 1024 context length with transformers by [SberDevices](https://sberdevices.ru/) team on 80B tokens around 3 epoch. After that model was finetuned on 2048 context.

Total training time took around one week on 32 GPUs.

You can obtain this model here [GDrive](https://drive.google.com/file/d/19dyhhayJSVJpVPwPzqLRIdCtOddvkzJ4/view?usp=sharing) or use in transformers with model name `sberbank-ai/rugpt3small_based_on_gpt2` (see [usage](#Usage-ruGPT3Small) for details). 

ü§óHuggingFace model card [link](https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2)

## Details of pretraining ruGPT2Large
Model was trained on 1024 context length with transformers by [SberDevices](https://sberdevices.ru/) team on 170Gb data on 64 GPUs 3 weeks.

You can obtain this model here [GDrive](https://drive.google.com/file/d/1r65MwU0arie8NggxpSmc_3Ja5ldRNS70/view?usp=sharing) [Yandex.Disk](https://yadi.sk/d/BRbn4fl9wqKy0w) [GDrive option-2](https://drive.google.com/file/d/17YuV-uuhSVvMD1cnTe7cR-qscb3BtTiG/view?usp=sharing) or use in transformers with model name `sberbank-ai/rugpt2large` (see [usage](#Usage-ruGPT2Large) for details).

ü§óHuggingFace model card [link](https://huggingface.co/sberbank-ai/rugpt2large)

# Usage
## Usage ruGPT3XL
See all details [here](gw/) or run example on [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/ruGPT3XL_generation.ipynb)

## Usage ruGPT3Large
We've provided 2 scripts that pretrain and generate with ruGPT3Large. Save and load model checkpoints with `--save` and `--load`.

### Finetuning
#### Data preparation
We support three file formats for training, but all require preprocessing. First, place your training data in a loose json format, with one json containing a text sample per line. For example:

```json
{"src": "KISH", "text": "–ö–∞–∫ –∂–µ –¥–∂–æ–∫–µ—Ä —Ç—ã —Ö–∏—Ç–µ—Ä", "type": "Ru", "id": "0", "title": "First Part"}
{"src": "The Internet", "text": "–¢—ã —É–¥–∞—á–∏ –ø—Ä–∏–≥–æ–≤–æ—Ä", "type": "Ru", "id": "42", "title": "Second Part"}
```

The name of the text field of the json can be changed by using the `--text-key` flag. The other metadata are optional and are not used in training.
#### Running script
`bash ./scripts/pretrain_ruGPT3Large.sh`

This script runs single gpu ruGPT3Large pretraining. This script contains command for running on [Christophari](https://sbercloud.ru/ru/christofari):

```bash
MP_SIZE=1
NUM_GPUS_PER_WORKER=1

mpirun --np ${NUM_GPUS_PER_WORKER} python pretrain_megatron.py \
       --train-data /home/jovyan/data/train.jsonl \
       --valid-data /home/jovyan/data/valid.jsonl \
       --test-data /home/jovyan/data/valid.jsonl \
       --save /home/jovyan/ruGPT3Large/checkpoints_${now}_${host} \
       --load /home/jovyan/ruGPT3Large \
       --tensorboard-dir /home/jovyan/ruGPT3Large/runs_${now}_${host} \
       --save-interval 500 \
       --eval-interval 500 \
       --log-interval 100 \
       --model-parallel-size ${MP_SIZE} \
       --num-layers 24 \
       --hidden-size 1536 \
       --num-attention-heads 16 \
       --seq-length 2048 \
       --max-position-embeddings 2048 \
       --vocab-size 50257 \
       --batch-size 1 \
       --train-iters 200000 \
       --distributed-backend nccl \
       --lr 0.00015 \
       --lr-decay-style cosine \
       --weight-decay 1e-2 \
       --clip-grad 1.0 \
       --warmup .01 \
       --fp16 \
       --lazy-loader \
       --checkpoint-activations \
       --loose-json \
       --text-key \
       --tokenizer-path /home/jovyan/ruGPT3Large \
       --tokenizer-type GPT2BPETokenizer \
       --finetune \
```

Or you can use use transformers interface:

```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("sberbank-ai/rugpt3large_based_on_gpt2")

model = AutoModel.from_pretrained("sberbank-ai/rugpt3large_based_on_gpt2")
```

### Text Generation
`bash ./scripts/generate_ruGPT3Large.sh`

Starts an interactive terminal session that generates text either conditionally or unconditionally depending on what the user enters into the prompt. 

The script is capable of top-k, or top-p sampling as specified by the appropriate variables within the script.

Example of generation:

```text
Context: –Ω–∞ —Å–ª–æ–≤–∞—Ö —Ç—ã –ª–µ–≤ —Ç–æ–ª—Å—Ç–æ–π
ruGPT3Large: –∞ –≤ —Å—É—â–Ω–æ—Å—Ç–∏, - —Ç—ã —Ç–æ–∂–µ –Ω–µ –¥—É—Ä–∞–∫, –ø—Ä–æ—Å—Ç–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –∏ —Ç–≤–æ–π —á–µ–ª–æ–≤–µ–∫, —Ç–æ –µ—Å—Ç—å —Ç–≤–æ—è "–∂–∏–∑–Ω—å", –∞ —Ç–∞–∫–∂–µ –∫–∞–∫ –∏ —Ç—ã –¥—É–º–∞–µ—à—å –ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É "—Ç—ã" –∏ –µ—Å—Ç—å —Ç–≤–æ–∏ "–∂–∏–∑–Ω—å" –∏–ª–∏ "–≤—ã–±–æ—Ä" –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ —Ç–≤–æ–µ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è.

Context: –∫–∞–∫ –∂–µ –¥–∂–æ–∫–µ—Ä —Ç—ã —Ö–∏—Ç–µ—Ä
ruGPT3Large: –∏–ª–∏ –∞–≤—Ç–æ—Ä –∫–Ω–∏–≥–∏ –ø–æ –±–∏–∑–Ω–µ—Å—É!
```

Example of generation in colab [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/ruGPT3_generation_example.ipynb)

## Usage ruGPT3Medium
You can run megatron script with option `--load-openai` or use transformers interface:

```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("sberbank-ai/rugpt3medium_based_on_gpt2")

model = AutoModel.from_pretrained("sberbank-ai/rugpt3medium_based_on_gpt2")
```

### Text Generation
`bash ./scripts/generate_ruGPT3Medium.sh`

Starts an interactive terminal session that generates text either conditionally or unconditionally depending on what the user enters into the prompt. 

The script is capable of top-k, or top-p sampling as specified by the appropriate variables within the script.

Example of generation:

```text
Context >>> –ù–∞ —Å–ª–æ–≤–∞—Ö —Ç—ã –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π, –∞ –Ω–∞ –¥–µ–ª–µ
ruGPT: –ù–∞ —Å–ª–æ–≤–∞—Ö —Ç—ã –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π, –∞ –Ω–∞ –¥–µ–ª–µ —è ‚Äî –õ–µ–≤ –î–∞–≤–∏–¥–æ–≤–∏—á –¢—Ä–æ—Ü–∫–∏–π, ‚Äî —Å–∫–∞–∑–∞–ª —è. ‚Äî –¢–∞–∫ —á—Ç–æ –º—ã –µ—â–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º

Context: –∫–∞–∫ –∂–µ –¥–∂–æ–∫–µ—Ä —Ç—ã —Ö–∏—Ç–µ—Ä
ruGPT: –∫–∞–∫ –∂–µ –¥–∂–æ–∫–µ—Ä —Ç—ã —Ö–∏—Ç–µ—Ä, –≤ —ç—Ç–æ–π –∏–≥—Ä–µ
 - –Ø –Ω–µ –∑–ª–æ–¥–µ–π, –ø—Ä–æ—Å—Ç–æ —Ö–æ—Ç–µ–ª —É–∑–Ω–∞—Ç—å, –º–æ–∂–Ω–æ –ª–∏ —É–∑–Ω–∞—Ç—å –æ —á—ë–º?
```

## Usage ruGPT3Small
You can run megatron script with option `--load-openai` or use transformers interface:

```python
from transformers import AutoTokenizer, AutoModelWithLMHead

tokenizer = AutoTokenizer.from_pretrained("sberbank-ai/rugpt3small_based_on_gpt2")

model = AutoModelWithLMHead.from_pretrained("sberbank-ai/rugpt3small_based_on_gpt2")
```

### Text Generation
`bash ./scripts/generate_ruGPT3Small.sh`

Starts an interactive terminal session that generates text either conditionally or unconditionally depending on what the user enters into the prompt. 

The script is capable of top-k, or top-p sampling as specified by the appropriate variables within the script.

Example of generation:

```text
Context >>> –ù–∞ —Å–ª–æ–≤–∞—Ö —Ç—ã –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π, –∞ –Ω–∞ –¥–µ–ª–µ
ruGPT: –ù–∞ —Å–ª–æ–≤–∞—Ö —Ç—ã –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π, –∞ –Ω–∞ –¥–µ–ª–µ ‚Äì –¢–æ–ª—Å—Ç–æ–π, ‚Äì —Å —É–ª—ã–±–∫–æ–π –∑–∞–º–µ—Ç–∏–ª –ù–∏–∫–æ–ª–∞–π, ‚Äì —è –≤–∏–∂—É, —á—Ç–æ —Ç—ã –ø—Ä–∞–≤.

‚Äì –ê –≤–æ—Ç —ç—Ç–æ ‚Äì –¥—Ä—É–≥–æ–µ –¥–µ–ª–æ, ‚Äì —Å–∫–∞–∑–∞–ª –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π, ‚Äì —ç—Ç–æ –¥–µ–ª–æ –¥—Ä—É–≥–æ–µ.

‚Äì –î–∞, –¥–∞, ‚Äì —Å–æ–≥–ª–∞—Å–∏–ª—Å—è –ù–∏–∫–æ–ª–∞–π, ‚Äì —è –ø—Ä–∞–≤.

‚Äì –ê –≤–æ—Ç —á—Ç–æ, –õ–µ–≤ –ù–∏–∫–æ–ª–∞–µ–≤–∏—á, ‚Äì —Å–∫–∞–∑–∞–ª –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π, ‚Äì —è –¥—É–º–∞—é, —á—Ç–æ –≤ —ç—Ç–æ–º –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ —É –º–µ–Ω—è –Ω–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω–∏–π —Å–æ–º–Ω–µ–≤–∞—Ç—å—Å—è –≤ —Ç–≤–æ–µ–π –ø—Ä–∞–≤–æ—Ç–µ.
```

Example of finetune on essays and generation in colab [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/Finetune_ruGPT3Small.ipynb)

## Usage ruGPT2Large
We've provided 2 scripts that pretrain and generate with ruGPT2Large from [transformers](https://github.com/huggingface/transformers/tree/v2.8.0) original code.

### Finetuning
#### Data preparation
We can pass to model raw text files.
#### Running script
`bash ./scripts/pretrain_ruGPT2Large.sh`

This script runs single gpu ruGPT3Large pretraining. This script contains command for running on [Christophari](https://sbercloud.ru/ru/christofari):

```
python pretrain_transformers.py \
    --output_dir=/home/jovyan/rugpt2large/checkpoints_"${now}"_"${host}" \
    --model_type=gpt2 \
    --model_name_or_path=/home/jovyan/gpt2_large_bbpe_v50 \
    --do_train \
    --train_data_file=/home/jovyan/data/train.txt \
    --do_eval \
    --eval_data_file=/home/jovyan/data/valid.txt \
    --fp16
```

Or use transformers interface:

```
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("sberbank-ai/rugpt2large")

model = AutoModel.from_pretrained("sberbank-ai/rugpt2large")
```

### Text Generation
`bash ./scripts/generate_ruGPT2Large.sh`

Starts an interactive terminal session that generates text either conditionally or unconditionally depending on what the user enters into the prompt. 

The script is capable of top-k, or top-p sampling as specified by the appropriate variables within the script.

Example of generation:

```
Context: –ù–∞ —Å–ª–æ–≤–∞—Ö —Ç—ã –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π, –∞ –Ω–∞ –¥–µ–ª–µ
ruGPT: –ù–∞ —Å–ª–æ–≤–∞—Ö —Ç—ã –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π, –∞ –Ω–∞ –¥–µ–ª–µ ‚Äì –∫–æ–∑–µ–ª!¬ª ‚Äì —Ç–∞–∫ —è –ø—Ä–æ —Å–µ–±—è –ø–æ–¥—É–º–∞–ª, –Ω–æ —Ä–µ—à–∏–ª –Ω–µ –æ—Ç–≤–µ—á–∞—Ç—å. –Ø –≤—Å—Ç–∞–ª, –ø–æ–∫–ª–æ–Ω–∏–ª—Å—è
```
